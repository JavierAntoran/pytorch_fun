{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time, sys\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from src.utils import *\n",
    "from src.read_mnist import read_train_dev_mnist\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# read data\n",
    "x_train, y_train, x_dev, y_dev = read_train_dev_mnist()\n",
    "print('  x_train: %s' % str(x_train.shape))\n",
    "print('  x_dev: %s' % str(x_dev.shape))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train config\n",
    "n_epochs = 100\n",
    "batch_size = 300\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# net dims\n",
    "input_dim = x_train.shape[1]\n",
    "nb_units = 500\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "y_dev = np.argmax(y_dev, axis=1)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# pytorch\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 500)\n",
    "        self.fc4 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # -----\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        # -----\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        # -----\n",
    "        x = self.fc4(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# train\n",
    "cprint('c','\\nTrain MNIST:')\n",
    "cost_train = np.zeros(n_epochs)\n",
    "cost_dev = np.zeros(n_epochs)\n",
    "err_dev = np.zeros(n_epochs)\n",
    "\n",
    "nb_samples_train = x_train.shape[0]\n",
    "nb_samples_dev = x_dev.shape[0]\n",
    "\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # ---- W\n",
    "    model.train()\n",
    "    tic = time.time()\n",
    "    for ind in generate_ind_batch(nb_samples_train, batch_size):\n",
    "        x, y = torch.from_numpy(x_train[ind]), torch.from_numpy(y_train[ind])\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        x, y = Variable(x), Variable(y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = F.nll_loss(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cost_train[i] += loss.data[0] / float(nb_samples_train) * float(len(ind))\n",
    "    toc = time.time()\n",
    "\n",
    "    # ---- print\n",
    "    print(\"it %d/%d, Jtr = %f, \" % (i, n_epochs, cost_train[i]), end=\"\")\n",
    "    cprint('r','   time: %f seconds\\n' % (toc - tic))\n",
    "\n",
    "    # ---- dev\n",
    "    if i % 5 == 0:\n",
    "        model.eval() # deterministic dropout\n",
    "        for ind in generate_ind_batch(nb_samples_dev, batch_size, random=False):\n",
    "            x, y = torch.from_numpy(x_dev[ind]), torch.from_numpy(y_dev[ind])\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            x, y = Variable(x), Variable(y)\n",
    "\n",
    "            out = model(x)\n",
    "            loss = F.nll_loss(out, y)\n",
    "\n",
    "            pred = out.data.max(1)[1] # get the index of the max log-probability\n",
    "            err = pred.ne(y.data).cpu().sum()\n",
    "\n",
    "            cost_dev[i] += loss.data[0]  / float(nb_samples_dev) * float(len(ind))\n",
    "            err_dev[i] += err / float(nb_samples_dev)\n",
    "\n",
    "        cprint('g','    Jdev = %f, err = %f\\n' % (cost_dev[i], err_dev[i]))\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# save model\n",
    "cprint('c','Writting model.dat')\n",
    "torch.save(model.state_dict(), 'model.dat') # only weights\n",
    "# torch.save(model, 'model.dat') # complete object\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------------------------------\n",
    "# fig cost vs its\n",
    "plt.figure()\n",
    "plt.plot(cost_train, 'r')\n",
    "plt.plot(range(0, n_epochs, 5), cost_dev[::5], 'bo--')\n",
    "plt.ylabel('J')\n",
    "plt.xlabel('it')\n",
    "plt.grid(True)\n",
    "# plt.show(block=False)\n",
    "plt.savefig('train_cost.png')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
